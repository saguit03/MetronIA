{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Beat tracking with time-varying tempo\n\nThis notebook demonstrates how to use the beat tracker in situations where the tempo\nmay change over time.\n\nBy default, the beat tracking algorithm [1]_ estimates a single tempo for the entire\nsignal, though it does tolerate small amounts of fluctuation around that tempo.\nThis is well suited for songs that have an approximately constant tempo, but where\nindividual beats may not be exactly spaced accordingly.\nIt is not well suited for songs that have radical shifts in tempo, e.g., entire sections\nthat speed up or slow down, or gradual accelerations over long periods of time.\n\nThe implementation in librosa (``librosa.beat.beat_track``) extends this algorithm\nto support different tempo estimates at each time point in the signal, as\ndemonstrated below.\n\n.. [1] Ellis, Daniel PW. \"Beat tracking by dynamic programming.\"\n       Journal of New Music Research 36, no. 1 (2007): 51-60.\n       http://labrosa.ee.columbia.edu/projects/beattrack/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Code source: Brian McFee\n# License: ISC\n\nimport numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A recording with dynamic tempo\nThe example recording included in this notebook consists of a drum beat that\ngradually increases in tempo from 30 BPM to 240 BPM over a 30-second time interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y, sr = librosa.load('audio/snare-accelerate.ogg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the spectrogram of this recording and listen to it.\nFrom the spectrogram, we can see that the spacing between drum beats becomes\nsmaller over time, indicating a gradual increase of tempo.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nmelspec = librosa.power_to_db(librosa.feature.melspectrogram(y=y, sr=sr), ref=np.max)\nlibrosa.display.specshow(melspec, y_axis='mel', x_axis='time', ax=ax)\nax.set(title='Mel spectrogram')\n\nAudio(data=y, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Static tempo beat tracking\nIf we run the beat tracker on this signal directly and sonify the result as a click\ntrack, we can see that it does not follow particularly well.\nThis is because the beat tracker is assuming a single (average) tempo across the\nentire signal.\nNote: by default, `beat_track` assumes that there is silence at the end of the\nsignal, and trims last beats to avoid false detections. Here, there is no silence\nat the end, so we set `trim=False` to avoid this effect.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tempo, beats_static = librosa.beat.beat_track(y=y, sr=sr, units='time', trim=False)\n\nclick_track = librosa.clicks(times=beats_static, sr=sr, click_freq=660,\n                             click_duration=0.25, length=len(y))\n\nprint(f\"Tempo estimate: {tempo[0]:.2f} BPM\")\nAudio(data=y+click_track, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic tempo\nInstead of estimating an aggregated tempo for the whole signal, we now estimate\na time-varying tempo. For this purpose, we pass `aggregate=None` to \n`librosa.feature.tempo`. The return value tempo_dynamic, is an array:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tempo_dynamic = librosa.feature.tempo(y=y, sr=sr, aggregate=None, std_bpm=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameter `std_bpm` is the standard deviation of the tempo estimator and \nhas a default value of 1. Here, we have increased `std_bpm` to 4.\nThis is to account for the broad range of tempo drift in this\nparticular recording (30 BPM to 240 BPM).\nWe can plot the dynamic and static tempo estimates to see how they differ.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\ntimes = librosa.times_like(tempo_dynamic, sr=sr)\nax.plot(times, tempo_dynamic, label='Dynamic tempo estimate')\nax.axhline(tempo, label='Static tempo estimate', color='r')\nax.legend()\nax.set(xlabel='Time (s)', ylabel='Tempo (BPM)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`librosa.feature.tempo` estimates tempo over a sliding window whose duration \n`ac_size` is equal to 8 seconds by default.\nIn this example, the result is not perfect: `tempo_dynamic` contains jagged and\nnonuniform steps.\nHowever, it does provide a rough picture of how tempo is changing over time.\n\nWe can now use this dynamic tempo in the beat tracker directly:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tempo, beats_dynamic = librosa.beat.beat_track(y=y, sr=sr, units='time',\n                                               bpm=tempo_dynamic, trim=False)\n\nclick_dynamic = librosa.clicks(times=beats_dynamic, sr=sr, click_freq=660,\n                               click_duration=0.25, length=len(y))\n\nAudio(data=y+click_dynamic, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Note that since we're providing the tempo estimates as input to the beat tracker,\nwe can ignore the first return value (`tempo`) which will simply be a copy of the\ninput (`tempo_dynamic`).)\n\nWe can visualize the difference in estimated beat timings as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nlibrosa.display.waveshow(y=y, sr=sr, ax=ax, color='k', alpha=0.7)\nax.vlines(beats_static, 0.75, 1., label='Static tempo beat estimates', color='r')\nax.vlines(beats_dynamic, -1, -0.75, label='Dynamic tempo beat estimates', color='b', linestyle='--')\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the figure above, we observe that `beats_dynamic`, the beat estimates derived\nfrom dynamic tempo, align well to the audio recording.\nOn the contrary, `beats_static` suffers from alignment problems.\nIndeed, it has too many detections towards the beginning of the recording\u2014where\nthe static tempo estimate is too fast\u2014and too few detections towards the end of\nthe recording\u2014where the static tempo estimate is too slow.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}